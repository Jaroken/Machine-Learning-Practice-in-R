---
title: "Practical Machine Learning"
author: "Kenneth Preston"
date: "November 28, 2018"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
    df_print: default
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(e1071)
library(caret)
```

# Practical Machine Learning in R

## Steps in ML
Question -> input data -> features -> alogrithms -> parameters -> evaluations

Question: what you want to answer
Input Data: the data you will be using. Data might have limitations
Features: calculating characteristics of the data (e.g. number of times a word appears)
Algorithms: which algorithms is used


## Order of importance 

question > data > features > algorithms
more data > better models

Algorithms matter less than you might think!

## Best Machine learning method is
- interpretable
- simple
- accurate
- Fast
- Scalable
Often you will trade off accuracy to optimize for the above characteristics

Accuracy is often not the most important, but gets focused on the most

## In and out of sample error

In sample error
- in the training set, the data you use to build your predictor
- generally this error is more optimistic 

Out of sample error
- Error you get on a new data set, sometimes called generalization error.
- You care about out of sample error more than in sample, in sample error is less than out of sample

In sample error is always lower than out of sample because of overfitting of the data resulting in capturing the signal and the noise, when we just want the signal. By capturing the noise you dont perform well on the new data with the predictor.

## Prediction study design

1. Define your error rate
2. Split data into: 
    - training, testing (or Dev et) , validation (optional) (or called Test set)
3. On the training set pick features
    - Use cross-validation
4. On the training set pick prediction function
    - Use cross-validation
5. if no validation
    - Apply 1x to test set
6. if validation 
    - Apply to test set and refine
    - Apply 1x to validation
    
Prediction benchmarks let you know what prediction to beat, say if you pick just 0's are you doing better than this?

Avoid small data sets when splitting up datasets. 

Splits to train, dev, test

larger datasets

60% training
20% test (dev)
20% validation (test)

medium datasets

60% training
40% test (dev)

If you have a small sample size
    - Do cross validation
    - Report caveat of small sample size
    
Remember the following
    - Put the test and validation sets aside and dont look at them
    - generally randomly sample training and test
    - If time series data split train and test in time chunks
    - All subset should reflect as much diversity as possible
        - Random assignment helps
        - You can try to balance features, but this is tricky
        
## Types of Errors

Positive = Identified 
Negative = Rejected

True Positive = correctly identified
False Positive = incorrectly identified
True Negative = Correctly rejected
False Negative = incorrectly rejected

Sensitivity = True Positive / (True Positive + False Negative)
Specificity = True Negative / (False Positive + True Negative)
Positive Predictive Value = True Positive / (True Positive + False Positive)
Negative Predictive Value = True Negative / (False Negative + True Negative)
Accuracy = (True Positive + True Negative)/ (True Positive + False Positive + False Negative + True Negative)

In rare events you need to be careful when evaluating your model.

For continuous data evaluate your model using:

Mean squared error (MSE)  1/n nSum (prediction - truth) squared
Root Mean squared error (RMSE) root of 1/n nSum (prediction - truth) squared  -- most common measure of error for continuous data

## Common errror measures
1. Mean squared error (or root mean squared error)
  - for continuous data, but is sensitive to outliers
2. Median absolute deviation
  - Continuous data, often more robust
3. Sensitivity (recall)
  - if you want few missed positives
4. Specificity
  - if you want few negatives called positives
5. Accuracy
  - Weights falase and positives/negatives equally
6. Concordance
  - One example is kappa (but whole bunch of others for multiclass data)
  
## ROC curves
reciever operating characteristic

y axis - True positive
x axis - False positive

How large the area under the curve is helps evaluate how good a predictor is

AUC = 0.5 is like random guessing
AUC = 1 is a perfect classifier
In general AUC of above 0.8 is considered goood


## Cross validation
One of the most widely used tools when evaluating a predictor and choosing features.

Approach:
1. Use the training set
2. Split it into training/test sets
3. Build a model on the trianing set
4. Evaluate on the test set
5. Repeat and average the estimated errors

Used for: 
1. Picking variables to include in the model
2. Picking the type of prediction function to use
3. Picking the parameters in the prediction function
4. Comparing different predictors

Random subsampling
Can be done with random subsampling of the training set into train and test (or dev)

K-fold
Split the dataset into K subsets and use n-1 for training and 1 for test (or dev)

Leave one out
Leaving out 1 case for test (dev) and the rest for training

Considerations
  - For time series data must be used in chunks
  - for k-fold cross validation
    - larger k = less bias, more variance
    - smaller k = more bias, less variance
  - Random sampling must be done without replacement
  - Random sampling with replacement is the bootstrap
    - Underestimates of the error
    - Can be corrected, but is complicated (0.632 Bootstrap)
  - If you cross-validate to pick predictors estimate you must estimate errors on independent data

## Caret package Introduction

Some preprocessing (cleaning)
- preProcess

Data Splitting 
- creatDataPartition
- createResample
- createTimeSlices

Training/testing functions
- train
- predict

Model Comparison
- confusion matrix

Machine Learning alogirthms in R
- Linear discriminate analysis
- Regression
- Naive Bayes
- Support vector machines
- Classification and regression trees
- Random forests
- Boosting
- etc


```{r, echo=TRUE}
library(caret)
library(kernlab)
data(spam)

inTrain <- caret::createDataPartition(y = spam$type, p = 0.75, list = FALSE) # p value is the % in train

training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(training)
```

Training the model
```{r, echo=TRUE, warning=FALSE}
set.seed(32343)
modelFit <- caret::train(type ~., data = training, method = "glm")
```

```{r, echo=TRUE}
modelFit
```

You can look at the final model like so...
```{r, echo=TRUE}
modelFit$finalModel
```    

To predict on the testing sample it is pretty straight forward.
```{r, echo=TRUE}
predictions <- stats::predict(modelFit, newdata= testing)
head(predictions)
```

Evaluating the prediction accuacy with the confusion matrix
```{r, echo=TRUE}
caret::confusionMatrix(predictions, testing$type) # provide the predictions, and the actual values

```

## Data Slicing

Setting up our datasets
```{r, echo=TRUE}
library(caret)
library(kernlab)
data(spam)

inTrain <- caret::createDataPartition(y = spam$type, p = 0.75, list = FALSE) # p value is the % in train

training <- spam[inTrain,]
testing <- spam[-inTrain,]
dim(training)

```

crossvalidation set up. Here first we use the training set only
```{r, echo=TRUE}
set.seed(32323)
folds <- caret::createFolds(y = spam$type, k = 10, list = TRUE, returnTrain = TRUE)
sapply(folds, length) # check length of samples

folds[[1]][1:10]
```

You can return only the test set data folded
```{r, echo=TRUE}
set.seed(32323)
folds <- caret::createFolds(y = spam$type, k = 10, list = TRUE, returnTrain = FALSE)
sapply(folds, length) # check length of samples

folds[[1]][1:10]
```

Here we do resampling, returning the folds of the entire dataset. You sample with replacement here:
```{r, echo=TRUE}
set.seed(32323)
folds <- caret::createResample(y = spam$type, times = 10, list = TRUE)
sapply(folds, length) # check length of samples

folds[[1]][1:10]
```

Time slices for time series data
```{r, echo=TRUE}
set.seed(32323)
tme <- 1:1000
folds <- caret::createTimeSlices(y = tme, initialWindow = 20, horizon = 10)
names(folds) # check length of samples

folds$train[[1]]
folds$test[[1]]
```

## Training Options

```{r, echo=TRUE, warning=FALSE}
library(caret)
library(kernlab)
data(spam)

inTrain <- caret::createDataPartition(y = spam$type, p = 0.75, list = FALSE) # p value is the % in train

training <- spam[inTrain,]
testing <- spam[-inTrain,]
modelFit <- caret::train(type ~., data = training, method = "glm")

```

Here are some training options

### Train
```{r, echo=TRUE}
?caret::train
```

metrix options for setting your train function include

continous outcomes
 - RMSE = Root mean squared error
 - RSquared = R2 from regression models
 
Categorical outcomes
 - Accuracy = Fraction correct
 - Kappa = A measure of concordance


### TrainControl
Allows you to be much more percise when training models
```{r, echo=TRUE}
args(trainControl)

```

trainControl resampling

method 
 - boot = bootstrapping
 - boot632 = bootstrapping with adjustment
 - cv = cross validation
 - repeatedcv = repeated cross validation
 - LOOCV = leave one out cross validation
 
number 
 - for boot/cross valdiation
 - number of subsamples to take
 
repeated
 - Number of times to repeate subsampling
 - if big this can slow some things down
 
Setting the seed
 - keep random draw the same for cv for example so you make the same sampling draws
 - important when sharing code so people get the same answers when running the code

## Plotting Predictors

Understanding how the data looks and interactes with eachother. Note: make your plots only in the training set. Do not explore the test set. 

Things to keep an eye on:
 - Imbalance in outcomes/predictors
 - Outliers
 - Groups of points not explained by a predictor
 - Skewed variables

```{r, echo = TRUE}
library(ISLR)
library(ggplot2)
library(caret)
data(Wage)
summary(Wage)

```

```{r, echo = TRUE}
inTrain <- createDataPartition(y = Wage$wage, p=0.7, list = FALSE)
training  <- Wage[inTrain,]
testing   <- Wage[-inTrain,]
dim(training)
dim(testing)


```
 
```{r, echo = TRUE}
caret::featurePlot(x = training[,c("age", "education", "jobclass")],
                   y = training$wage, 
                   plot = "pairs")
```
qplot
```{r, echo = TRUE}
ggplot2::qplot(age, wage, data = training)

```

qplot with colour
```{r, echo = TRUE}
ggplot2::qplot(age, wage, colour = jobclass, data = training)

```

Here we add regression smoothers
```{r, echo = TRUE}
qq <- ggplot2::qplot(age, wage, colour = education, data = training)
qq + geom_smooth(method = 'lm', formula = y ~ x)

```
tables
```{r, echo = TRUE}
cutWage <- Hmisc::cut2(training$wage, g = 3)
table(cutWage)


```
Box plots
```{r, echo = TRUE}
p1 <- ggplot2::qplot(cutWage, age, data = training, fill = cutWage, geom = c("boxplot"))

p2 <- ggplot2::qplot(cutWage, age, data = training, fill = cutWage, geom = c("boxplot", "jitter"))
gridExtra::grid.arrange(p1,p2,ncol=2)

```
Tables
```{r, echo = TRUE}
t1 <- table(cutWage, training$jobclass)
t1
prop.table(t1, 1) # gives proportion in each columns
```

Density plots. This shows where the bulk of the data falls.
```{r, echo = TRUE}
ggplot2::qplot(wage, colour = education, data = training, geom = "density")
```

## Basic Preprocessing

You might need to transform odd looking data to make it more useful for prediction alogirmthms. This can be very important when using model based approaches, such as linear regression and naive bayes.

Things to keep in mind: 
 - Training and test must be preprocessed in the same way. Caret does a lot of this under the hood for you.
 - Test transformations will likely be imperfect (especially if the test/training sets collected at different times).
 - Careful when transforming factor variables.
 - Binary variables are generally left as is, and continuous variables are transformed to be more normal looking.

Why preprocess?

Variables can be very skewed...
```{r, echo = TRUE}
library(caret)
library(kernlab)
data(spam)
inTrain <- createDataPartition(y=spam$type, p = 0.75, list = FALSE)
training <- spam[inTrain, ]
testing <- spam[-inTrain, ]
hist(training$capitalAve, main = "", xlab="ave. capital run length")
```

We can see the variable has a lot variance
```{r, echo = TRUE}
mean(training$capitalAve)
sd(training$capitalAve)
```

We can standardize by subtracting out the mean
```{r, echo = TRUE}
trainCapAve <- training$capitalAve
trainCapAveS <- (trainCapAve - mean(trainCapAve))/sd(trainCapAve)
mean(trainCapAveS)
sd(trainCapAveS)

```    

keep in mind we need to do this for the test set, but we must use the train mean and sd to do the transformation of the test data, like so...
```{r, echo = TRUE}
testCapAve <- testing$capitalAve
testCapAveS <- (testCapAve - mean(trainCapAve))/sd(trainCapAve) # notice trainCapAve is used here
mean(testCapAveS)
sd(testCapAveS)
```    

### preProcess

Can do a lot of the preprocessing for you would normally need to do by hand 
```{r, echo = TRUE, warning=FALSE}
preObj <- caret::preProcess(training[,-58], method=c("center", "scale"))
trainCapAveS <- predict(preObj, training[, -58])$capitalAve
mean(trainCapAveS)
sd(trainCapAveS)
```

You can use the preObj data to apply the preprocessing to the testing set. The mean will not be exactly 0, because it is based off of the training set
```{r, echo = TRUE, warning=FALSE}
testCapAveS <- predict(preObj, testing[,-58])$capitalAve
mean(testCapAveS)
sd(testCapAveS)
```

You can send the preprocessing commands directly to the train command in caret like so. This will center and scale all predictors when running 
```{r, echo = TRUE, warning=FALSE}
set.seed(32343)
modelFit <- train(type ~.,data = training, preProcess = c("center", "scale"), method = "glm")
modelFit
```

You can do other kinds of transformations, like Box-Cox tranformations. Takes continuous data and tries to make them look like normal data. They do this by estimating certain parameters using maximum likelihood. You will notice the data now looks more normal as a result. Handles skewness really well.
```{r, echo = TRUE, warning=FALSE}
preObj <- caret::preProcess(training[,-58], method=c("BoxCox"))
trainCapAveS <- predict(preObj, training[, -58])$capitalAve
par(mfrow = c(1,2))
hist(trainCapAveS)
qqnorm(trainCapAveS)
par(mfrow = c(1,1)) # setting back to 1 plot at a time
```

### Standardizing - Imputing Data

Imputing missing values using k nearest neighbours
```{r, echo = TRUE, warning=FALSE}
set.seed(13343)

# Make some values NA
training$capAve <- training$capitalAve
selectNA <- rbinom(dim(training)[1], size=1, prob=0.05)==1
training$capAve[selectNA] <- NA

# Impute and standardize
library(RANN) # needs this additional package to run the knn
preObj <- caret::preProcess(training[,-58], method="knnImpute") # finds the closest values and averages these to impute
capAve <- predict(preObj, training[,-58])$capAve

# Standardize true values
capAveTruth <- training$capitalAve
capAveTruth <- (capAveTruth - mean(capAveTruth))/sd(capAveTruth)
```

```{r, echo = TRUE, warning=FALSE}
quantile(capAve - capAveTruth)
quantile((capAve - capAveTruth)[selectNA]) # just the imputed values
quantile((capAve - capAveTruth)[!selectNA]) # not the ones selected to be NA

```

## Covariate (predictors/features) creation

Sometimes called predictors or features. These are the variables used to predict our dependent variable. This involves taking the raw variable, like image and text, and try to create variables from this for predictors/features/covariates. We do this so the variables can fit in standard ML algorithms.

Level 1 feature creation - Raw data to covariates
 - if in doubt google how to create features for you data type
 - err on overcreation of features
 - Deep learning can be helpful for automatic feature detection on things like images or voice
Level 2 feature creation - covariabes to new covariates
 - some of this handled by preProcess function in caret
 - create new covariates if you think they will improve fit
 - use exploratory analysis on training set for creating them
 - beware overfitting


You will want to pick the right features to include with minimizing information loss. 

```{r, echo = TRUE, warning=FALSE}
library(kernlab)
data(spam)
spam$capitalAveSq <- spam$capitalAve^2
```


```{r, echo = TRUE, warning=FALSE}
library(ISLR)
library(caret)
data(Wage)
inTrain <- createDataPartition(y = Wage$wage, p = 0.7, list = FALSE)
training <- Wage[inTrain, ]
testing <- Wage[-inTrain, ]
```

Common covariates to add include dummy variables for factor variables to transform them to indicator variables. Below outputs an indicator that you are either industrial or information class of job type.

```{r, echo = TRUE, warning=FALSE}
table(training$jobclass)
dummies <- caret::dummyVars(wage ~ jobclass, data = training)
head(predict(dummies, newdata = training))
```

Remove zero covariates

This can identify which variables are near zero. The nzv column can help us know which variables to throw out.

```{r, echo = TRUE, warning=FALSE}
nsv <- nearZeroVar(training, saveMetrics = TRUE)
nsv
```

Spline basis

This creates a 3 polynomial spline. First column will be the variable, 2nd will be the column squared, and the 3rd would be cubed. This is important for allowing for curvy model fitting.

```{r, echo = TRUE, warning=FALSE}
library(splines)
bsBasis <- bs(training$age, df = 3)
bsBasis
```

Here we fit more curvey data
```{r, echo = TRUE, warning=FALSE}
lm1 <- lm(wage ~ bsBasis, data = training)
plot(training$age, training$wage, pch = 19, cex=0.5)
points(training$age, predict(lm1, newdata = training), col = "red", pch = 19, cex = 0.5)
```
You will need to apply those Splines on the test set as well, You use the bsBasis function on the test set as opposed to applying the bs function again.
```{r, echo = TRUE, warning=FALSE}
predict(bsBasis, age = testing$age) # this creates
```

If you want to fit spline models, use the gam method in the caret package which allows smoothing of multiple variables. 

## Preprocessing with PCA

Often you have multiple variables with high covariance, you may want to combine some of these variables to get a summary that captures the important varianace. 

```{r, echo = TRUE, warning=FALSE}
library(kernlab)
library(caret)
data(spam)
inTrain <- createDataPartition(y = spam$type, p = 0.75, list = FALSE)
training <- spam[inTrain, ]
testing <- spam[-inTrain, ]

M <- abs(cor(training[,-58]))
diag(M) <- 0
which(M > 0.8, arr.ind = TRUE)
```
We can see that these two variables are highly correlated with one another. We might find including both of these variables will not be very useful for out model.
```{r, echo = TRUE, warning=FALSE}
names(spam[c(34, 32)])
plot(spam[,34], spam[, 32])
```

A weighted combination of predictors might be better. We should pick this combination to capture the most info possible. The benefits of this is a reduced number of predictors and reduced noise (due to averaging).

Statistical - We will want to do this to generate a set of multivariate variables that are uncorrelated and explain as much variance as possible. 

Data Compression - We also want to have less predictor variables for better data compression.

### two solutions

SVD 

PCA

 - most useful for linear type models - linear discriminate, glm
 - Can make it harder to interpret predictors
 - watch out of outliers! 
    - transfrom first (with logs/Box Cox)
    - plot predictors to identify problems

```{r, echo = TRUE, warning=FALSE}
smallSpam <- spam[,c(34, 32)]
prComp <- prcomp(smallSpam)
plot(prComp$x[,1], prComp$x[,2])
```
Allows you to look at many variables and condense them down quite abit

This lets you see how it is summing up the variables
```{r, echo = TRUE, warning=FALSE}
prComp$rotation
```

Here we do pca on more variables..
```{r, echo = TRUE, warning=FALSE}
typeColor <- ((spam$type == "spam")*1 + 1)
prComp <- prcomp(log10(spam[,-58]+1)) # added in the log10 and +1 to make the data look more normal. You often need to do this for PCA
plot(prComp$x[,1], prComp$x[,2], col = typeColor, xlab = "PC1", ylab = "PC2")
```

We can do this PCA in caret as well
```{r, echo = TRUE, warning=FALSE}
preProc <- caret::preProcess(log10(spam[,-58]+1), method = "pca", pcaComp = 2) # notice inclduing the log10 + 1, here we say calculate 2 principal components
spamPC <- predict(preProc, log10(spam[,-58]+1)) # we use the predict function to calculate the PCA
plot(spamPC[,1], spamPC[,2], col = typeColor)
```

Training a model with PCA preprocessing
```{r, echo = TRUE, warning=FALSE}
data(spam)
inTrain <- createDataPartition(y=spam$type,p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
# create preprocess object
preProc <- preProcess(log10(training[,-58]+1),method="pca",pcaComp=2)
# calculate PCs for training data
trainPC <- predict(preProc,log10(training[,-58]+1))
modelFit <- train(x = trainPC, y = training$type, method="glm")
```

Testing the model with PCA preprocessing
```{r, echo = TRUE, warning=FALSE}
testPC <- predict(preProc, log10(testing[,-58]+1)) # notice here we use the same preProc object created off the training set
confusionMatrix(testing$type, predict(modelFit, testPC))
```

## Predicting with regression

Key ideas
 - fit a simple regression model
 - plug in new covariates and multiple by coefficients
 - useful when linear model is nearly correct
 
Pros
 - easy to implement
 - easy to interpret

Cons
 - Often poor performance in nonlinear settings
 
```{r, echo = TRUE, warning=FALSE}
library(caret)
data(faithful)
set.seed(333)

inTrain <- caret::createDataPartition(y = faithful$waiting, p = 0.5, list = FALSE)
trainFaith <- faithful[inTrain, ]
testFaith <- faithful[-inTrain, ]
head(trainFaith)

```

```{r, echo = TRUE, warning=FALSE}
plot(trainFaith$waiting, trainFaith$eruptions, pch = 19, col = "blue", xlab = "Waiting", ylab = "Duration")
```

fit a linear model
```{r, echo = TRUE, warning=FALSE}
lm1 <- lm(eruptions ~ waiting, data = trainFaith)
summary(lm1)
```

```{r, echo = TRUE, warning=FALSE}
plot(trainFaith$waiting, trainFaith$eruptions, pch = 19, col = "blue", xlab = "Waiting", ylab = "Duration")
lines(trainFaith$waiting, lm1$fitted.values, lwd=3)
```

Predicting a new variable
```{r, echo = TRUE, warning=FALSE}
coef(lm1)[1] + coef(lm1)[2]*80
newdata <- data.frame(waiting = 80) # you can use the predict command to do the same as above
predict(lm1, newdata)
```

You can see that the prediction doesnt fit the test set exactly (right side), but it does capture the trend.
```{r, echo = TRUE, warning=FALSE}
par(mfrow = c(1,2))
plot(trainFaith$waiting, trainFaith$eruptions, pch=19, col="blue", xlab = "Waiting", ylab = "Duration")
lines(trainFaith$waiting, predict(lm1), lwd = 3)
plot(testFaith$waiting, testFaith$eruptions, pch=19, col="blue", xlab = "Waiting", ylab = "Duration")
lines(testFaith$waiting, predict(lm1, newdata=testFaith), lwd=3)
par(mfrow = c(1,1))
```

Next, we get training set/test set errors
```{r, echo = TRUE, warning=FALSE}
# Calculate RMSE on training
sqrt(sum((lm1$fitted.values-trainFaith$eruptions)^2))

# Calculate RMSE on test
sqrt(sum((predict(lm1, newdata = testFaith) - testFaith$eruptions)^2))
```
The test dataset error is almost always more error.

Prediction intervals
Interval for where we expect the values to fall
```{r, echo = TRUE, warning=FALSE}
pred1 <- predict(lm1, newdata = testFaith, interval = "prediction")
ord <- order(testFaith$waiting)
plot(testFaith$waiting, testFaith$eruptions, pch = 19, col = "blue")
matlines(testFaith$waiting[ord], pred1[ord,], type = "l", col = c(1,2,2), lty = c(1,1,1), lwd=3)
```

You can do the same thing in the caret package
```{r, echo = TRUE, warning=FALSE}
modFit <- train(eruptions ~ waiting, data = trainFaith, method = "lm")
summary(modFit$finalModel)
```

Regression can be combined with other models.

## Predicting with Multivariate Regression 

Most importantly we will be exploring which are the best variables to include within a prediction model.

Regression is useful with linear relationships and is often useful in combination with other models.

```{r, echo = TRUE, warning=FALSE}
library(ISLR)
library(ggplot2)
library(caret)
data(Wage)
Wage <- subset(Wage,select = -c(logwage)) # removing variable we want to predict
summary(Wage)
```

```{r, echo = TRUE, warning=FALSE}
inTrain <- createDataPartition(y = Wage$wage, p = 0.7, list = FALSE)
training <- Wage[inTrain, ]
testing <- Wage[-inTrain, ]
dim(training)
dim(testing)
```

feature plot
```{r, echo = TRUE, warning=FALSE}
featurePlot(x = training[,c("age", "education", "jobclass")], y = training$wage, plot = "pairs")
```
plot variables versus wage
```{r, echo = TRUE, warning=FALSE}
qplot(age, wage, data = training)
```

```{r, echo = TRUE, warning=FALSE}
qplot(age, wage, colour = jobclass, data = training)
```

```{r, echo = TRUE, warning=FALSE}
qplot(age, wage, colour = education, data = training)
```
fit multivariate regression model. Factor variables are automatically transformed to indicators.
```{r, echo = TRUE, warning=FALSE}
modFit <- train(wage ~ age + jobclass + education, method = "lm", data = training)
finMod <- modFit$finalModel
print(modFit)
```
Diagnostic plots

Identifies outliers and how lcosely we fit the data
```{r, echo = TRUE, warning=FALSE}
plot(finMod, 1, pch=19, cex=0.5, col='#00000010')
```

```{r, echo = TRUE, warning=FALSE}
qplot(finMod$fitted.values, finMod$residuals, colour = race, data = training)
```
plot by index
```{r, echo = TRUE, warning=FALSE}
plot(finMod$residuals, pch=19)
```
Predicted versus truth in test set
```{r, echo = TRUE, warning=FALSE}
pred <- predict(modFit, testing)
qplot(wage, pred, colour = year, data= testing)
```
You cannot use this analysis to change your model. This is post analysis.

if you want to use all covariates in your model you can
```{r, echo = TRUE, warning=FALSE}
modFitAll <- train(wage ~ ., data = training, method = "lm")
pred <- predict(modFitAll, testing)
qplot(wage, pred, data = testing)
```

## Predicting with Trees

Splitting variables into groups. It is easier to interpret and performs better in non-linear settings. The cons are without pruning/cross-validation can lead to overfitting. It can also be hard to estimate uncertainty and results may be variable.

Basic Algorithm
1. start with all variables in one group
2. find the variable/split that best seperates the outcomes
3. divide the data into two groups("leaves") on that split ("node")
4. Within each split, find the best variable/split that seperates the outcomes
5. Continue until the groups are too small or sufficiently "pure"

Measures of impurtity
 - Missclassification error (0 = perfect purity, 0.5 = no purity)
 - Gini Index (0 = perfect purity, 0.5 = no purity)
 - Deviance/information gain (0 = perfect purity, 1 = no purity)

Note: 
 - Classifcation trees are non-linear models
  - They use interactions between variables
  - Data transformations may be less important (monotone transformation)
  - Trees can also be used for regression problems (continuous outcome)
 - Note there are multiple tree building options in R both in the caret package - party, rpart, and out of caret package - tree

```{r, echo = TRUE, warning=FALSE}
 data(iris)
 library(ggplot2)
 names(iris)
 table(iris$Species)
```

```{r, echo = TRUE, warning=FALSE}
inTrain <- caret::createDataPartition( y = iris$Species, p = 0.7, list = FALSE)
training <- iris[inTrain, ]
testing <- iris[-inTrain, ]
dim(training)
dim(testing)

```

```{r, echo = TRUE, warning=FALSE}
qplot(Petal.Width, Sepal.Width, colour = Species, data = training)
```

```{r, echo = TRUE, warning=FALSE}
modFit <- caret::train(Species ~ ., method = "rpart", data = training)
print(modFit$finalModel)
```

```{r, echo = TRUE, warning=FALSE}
plot(modFit$finalModel, uniform = TRUE, main = "Classification Tree")
text(modFit$finalModel, use.n= TRUE, all = TRUE, cex = .8)
```

This code will make a pretty version of the tree chart. It works on my local environment, but not in the shiny linux environment where it is hosted.
```{r, echo = TRUE, warning=FALSE}
# library(rattle)
# fancyRpartPlot(modFit$finalModel)
```


```{r, echo = TRUE, warning=FALSE}
predict(modFit, newdata = testing)
```

## Bagging - Bootstrap Aggregating
Average of complicated models gives better balance between bias and variance in your fit.

Basic Idea 
 1. Resample cases and recalculate predictions
 2. Average or majority vote
 
Notes
 - similar bias
 - reduced variance
 - more useful for non-linear functions
 
```{r, echo = TRUE, warning=FALSE}
library(ElemStatLearn)
data(ozone, package = "ElemStatLearn")
ozone <- ozone[order(ozone$ozone), ]
head(ozone)
```

```{r, echo = TRUE, warning=FALSE}
ll <- matrix(NA, nrow=10, ncol =155)
for(i in 1:10){
  ss <- sample(1:dim(ozone)[1], replace = TRUE)
  ozone0 <- ozone[ss,]
  ozone0 <- ozone0[order(ozone0$ozone), ]
  loess0 <- loess(temperature ~ ozone, data = ozone0, span = 0.2) # fits a smooth curve through the data similar to splines
  ll[i,] <- predict(loess0, newdata=data.frame(ozone=1:155))
}
```

```{r, echo = TRUE, warning=FALSE}
plot(ozone$ozone, ozone$temperature, pch=19, cex=0.5)
for(i in 1:10){lines(1:155, ll[i,], col = "grey", lwd = 2)}
lines(1:155, apply(ll, 2, mean), col = "red", lwd=2)
```

### Bagging in caret

some models perform bagging for you, in train function consider method options: bagEarth, treebag, bagFDA. alternatively you can choose using the bag function. 

 - Bagging is most useful for nonlinear models
 - often used with trees - an extension is random forest
 - several models use baggin in caret's train function

Advanced bagging in caret
```{r, echo = TRUE, warning=FALSE}
predictors = data.frame(ozone$ozone)
temperature = ozone$temperature
treebag <- bag(predictors, temperature, B = 10, # number of subsamples
               bagControl =  # how you will control bagging
                 bagControl(fit = ctreeBag$fit,
                            predict = ctreeBag$pred,
                            aggregate = ctreeBag$aggregate))
```

```{r, echo = TRUE, warning=FALSE}
plot(ozone$ozone, temperature, col = 'lightgrey', pch=19)
points(ozone$ozone, predict(treebag$fits[[1]]$fit, predictors), pch =19, col = "red")
points(ozone$ozone, predict(treebag, predictors), pch=19, col = "blue")
```

```{r, echo = TRUE, warning=FALSE}
ctreeBag$fit
ctreeBag$pred
ctreeBag$aggregate
```

## Random Forest

Random forests are usually one of the two top performing algorithms along with boosting in prediction contests. Random Forests are difficult to interpret but often are very accurate. Care should b etaken to avoid overfitting (see rfcv function)

Extension of bagging. Rebuilding regression or classification trees. It is an ensemble model.

Steps:
 1. Bootstrap samples
 2. At each split, bootstrap variables
 3. grow multiple trees and vote
 
Pro:
 1. Accuaracy
 
Cons:
 1. Speed
 2. Interpretability
 3. Overfitting (very important to use cross validation when using random forest)
 
```{r, echo = TRUE, warning=FALSE}
data(iris)
library(ggplot2)
inTrain <- caret::createDataPartition( y = iris$Species, p = 0.7, list = FALSE)
training <- iris[inTrain, ]
testing <- iris[-inTrain, ]
```

```{r, echo = TRUE, warning=FALSE}
modFit <- caret::train(Species ~ ., data = training, method = "rf", prox = TRUE)
modFit
```

```{r, echo = TRUE, warning=FALSE}
randomForest::getTree(modFit$finalModel, k = 2)
``` 

```{r, echo = TRUE, warning=FALSE}
irisP <- randomForest::classCenter(training[,c(3,4)], training$Species, modFit$finalModel$proximity)
irisP <- as.data.frame(irisP); irisP$Species <- rownames(irisP)
p <- qplot(Petal.Width, Petal.Length, col=Species, data = training)
p + geom_point(aes(x=Petal.Width, y=Petal.Length, col = Species), size=5, shape=4, data=irisP)
```

```{r, echo = TRUE, warning=FALSE}
pred <- predict(modFit, newdata = testing)
testing$predRight <- pred==testing$Species
table(pred, testing$Species)
```

```{r, echo = TRUE, warning=FALSE}
qplot(Petal.Width, Petal.Length, colour = predRight, data = testing, main="newdata Predictions")
```

### Boosting
along with random forest, this is one of the most accurate out of the box classifiers you can use.

Basic idea
1. take lots of (possibly) weak predictors
2. weight them and add them up
3. get a stronger predictor

Steps 
1. start with a set of classifiers
  - examples: all possible trees, all possible regression models, all possible cutoffs
2. Create a classifier that combines classification functions
  - Goal is to minimize error (on training set)
  - iterative, select one h at each step
  - calculate weights based on errors
  - Upweight missed classifications and select next h
  
Adaboost is one of the most popular boosting function.

We creat a classifier... upweight misclassified points... reclassify... repeat

Boosting can be used with any subset of classifiers
One large subclass is gradient boosting
R has multiple boosting libraries. differences include the choice of basic classification functions and combined rules
- gbm - boosting of trees
- mboost - model based boosting
- ada - statistical boosting based on additive logistic regression
- gamBoost for boosting generalized additive models
??xgboost
Most of these are available in the caret package

```{r, echo = FALSE, warning=FALSE}
library(ISLR)
data(Wage)
library(ggplot2)
library(caret)
Wage <- subset(Wage, select=-c(logwage))
inTrain <- createDataPartition(y = Wage$wage,
                               p=0.7, 
                               list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]

```

```{r, echo = FALSE, warning=FALSE}
modFit <- train(wage ~ ., method = "gbm", data=training, verbose = FALSE) # boosting with trees - boosted version of regression trees
print(modFit)
```

```{r, echo = FALSE, warning=FALSE}
qplot(predict(modFit, testing), wage, data=testing)

```


## Model Based Prediction 

Basic idea
1. Assume the data follow a probabilistic model
2. Use Bayes' theorem to identify optimal classifiers

Pros
 - Can take advantage of structure of the data
 - May be computationally convienent
 - Are reasonably accurate on real problems
 
Cons 
 - Make additional assumptions about the data
 - When the model is incorrect you may get reduced accuracy
 
Model based approach
1. Our goal is to build a parametric mode for conditional distribution
2. A typical approach is to apply Bayes theorem
3. Typically prior probabilies are set in advance
4. A common choice for the distribution is Gaussian 
5. Estimate the parameters from the data
6. Classify to the class with the highest values for the conditional distribution

A range of models use this approach
 - Linear discriminant analysis assumes distribution is multivariate gaussian with same covariances
 - Quadratic discriminat analysis assumes distribution is multivariate gaussian with different covariances
 - model based prediction assumes more complicated versions for the covariance matrix
 - Naive Bayes assumes independence between features for model building

Discriminant function
- decide on class based on Y
- we usually estimate parameters with maximum likelihood

Naive Bayes
- supose we have many predictors, we would want to model Probability of y given X...
- We could use bayes theorem to get...


```{r, echo = FALSE, warning=FALSE}
data(iris)
library(ggplot2)
table(iris$Species)
inTrain <- createDataPartition(y = iris$Species, p = 0.7, list = FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training)
dim(testing)

modlda <- train(Species ~., data = training, method = "lda") # lda model
modnb <- train(Species ~., data = training, method = "nb") # naive bayes
plda <- predict(modlda, testing)
pnb <- predict(modnb, testing)
table(plda, pnb)
```

```{r, echo = FALSE, warning=FALSE}
equalPredictions <- (plda == pnb)
qplot(Petal.Width, Sepal.Width, colour = equalPredictions , data = testing)
```

## Regularized Regression

Basic Idea
1. fit a regression model
2. Penalize or shrink large coefficients

Pros:
 - helps with bias/variance tradeoff
 - can help with model selection
 
Cons:
 - May be computationally demanding on large data sets
 - Does not perfroma as well as random forests and boosting
 
```{r, echo = FALSE, warning=FALSE}
library(ElemStatLearn)
data(prostate)
str(prostate)
```

Model Selection approach: split samples
 - no method better when data/computation time permits it
 - Approach 
  1. Divide dat ainto training/test/validation
  2. Treat validation as test data, train all competing modes on the train and pick the best one on validation
  3. To appropriately assess performance on new data apply to test set 
  4. You may re-split and reperform steps 1-3
  
 - two common problems
   - limited data
   - computational complexity
   
Decomposing expected prediction error

```{r, echo = FALSE, warning=FALSE}
small <- prostate[1:5,]
lm(lpsa ~.,data = small)
```

Regularization for regression

We would like to penalize high coefficients
 - penalty reduces complexity
 - penalty reduces variance
 - penalty respects structure of the problem
 
### Ridge regression

- increasing lambda shrinks the coefficents towards 0
- included in caret

### Lasso

- lassos strinks some variables coefficents to zero doing feature seletion for you
- included in caret

## Combining predictors (ensemble methods)

 - you can combine classifiers by averaging/voting
 - combining classifers improves accuracy
 - combining classifier reduces interpretability
 - Boosting, bagging, and random forests are variants on this theme

Basic intuition - majority vote

- Suppose we have 5 completely independent classifiers
    - if accuracy is 70% for each we get 83.7% majority vote accuracy
- With 101 independent classifiers we get 99.9% accuracy

Approaches for combining classifiers

1. Bagging, boosting, random forests
  - Usually combine similar classifers
2. Combining different classifiers
  - Model stacking
  - Model ensembling
  
Notes: 
 - Even simple blending can be useful
 - Typical model for binary/multiclass data
    - Build an odd number of models
    - Predict with each models
    - Predict the class by majority vote
 - This can get dramatically more complicated
    - Simple blending in caret: caretEnsemble (use at your own risk!)
    
### Example of model stacking
```{r, echo = FALSE, warning=FALSE}
library(ISLR)
data(Wage)
library(ggplot2)
library(caret)
Wage <- subset(Wage, select=-c(logwage))

# create a building data set and valdiation set

inBuild <- createDataPartition(y = Wage$wage,
                               p = 0.7, 
                               list = FALSE)
validation <- Wage[-inBuild,]
buildData <- Wage[inBuild,]
inTrain <- createDataPartition(y=buildData$wage,
                               p = 0.7, list = FALSE)
training <- buildData[inTrain, ]
testing <- buildData[-inTrain, ]

dim(training)
dim(validation)
dim(testing)
```

Build two different models
```{r, echo = FALSE, warning=FALSE}
mod1 <- train(wage ~., method = "glm", data = training)
mod2 <- train(wage ~., method = "rf", data = training, trControl = trainControl(method="cv"), number=3)
```

```{r, echo = FALSE, warning=FALSE}
pred1 <- predict(mod1, testing)
pred2 <- predict(mod2, testing)
qplot(pred1, pred2, colour=wage, data=testing)
```

fit a model that combines predictors
```{r, echo = FALSE, warning=FALSE}
predDF <- data.frame(pred1, pred2, wage = testing$wage)
combModFit <- train(wage ~., method = "gam", data=predDF)
combPred <- predict(combModFit, predDF)
```

```{r, echo = FALSE, warning=FALSE}
sqrt(sum((pred1 - testing$wage)^2))
sqrt(sum((pred2 - testing$wage)^2))
sqrt(sum((combPred - testing$wage)^2))

```

```{r, echo = FALSE, warning=FALSE}
pred1V <- predict(mod1, validation)
pred2V <- predict(mod2, validation)
predVDF <- data.frame(pred1 = pred1V, pred2 = pred2V)
combPredV <- predict(combModFit, predVDF)
```

Evaluation of validation
```{r, echo = FALSE, warning=FALSE}
sqrt(sum((pred1V - validation$wage)^2))
sqrt(sum((pred2V - validation$wage)^2))
sqrt(sum((combPredV - validation$wage)^2))

```

## Forecasting
 - Data are dependent over time
 - Specific pattern types
    - Trends - long term increase or decrease
    - Seasonal patterns - patterns related to time of week, month, year, etc
    - Cyclces- patterns that rise and fall periodically
 - subsampling into training/test is more complicated
 - similar issues arise in spatial data
    - Dependency between nearby observations
    - Location specific effects
 - Typically goal is to predict one or more observation into the future
 - All standard predictions can be used (with caution!)

## Unsupervised Prediction: K-means

- Unlabelled data

```{r, echo = TRUE, warning=FALSE}
 data(iris)
 library(ggplot2)
 names(iris)
 table(iris$Species)
```

```{r, echo = TRUE, warning=FALSE}
inTrain <- caret::createDataPartition( y = iris$Species, p = 0.7, list = FALSE)
training <- iris[inTrain, ]
testing <- iris[-inTrain, ]
dim(training)
dim(testing)

```

```{r, echo = TRUE, warning=FALSE}
kMeans1 <- kmeans(subset(training, select =-c(Species)), centers=3)
training$clusters <- as.factor(kMeans1$cluster)
qplot(Petal.Width, Petal.Length, colour=clusters, data=training)
```

```{r, echo = TRUE, warning=FALSE}
table(kMeans1$cluster, training$Species)
```

```{r, echo = TRUE, warning=FALSE}
modFit <- train(clusters ~., data = subset(training, select=-(Species)), method = "rpart")
table(predict(modFit, training), training$Species)
```

```{r, echo = TRUE, warning=FALSE}
testClusterPred <- predict(modFit, testing)
table(testClusterPred, testing$Species)
```

This is an exploratory analysis technique so keep this in mind when interpreting the clusters. This is actually one basic approach to recommendation engines.